---
title: "Assignment 8"
format: html
editor: visual
---

## Part 1: Run LDA Lab

```{r}
# install and load pacman for package management
if (!require("pacman", character.only = TRUE)) install.packages("pacman")
library(pacman)
# load libraries using pacman
p_load("ISLR", "MASS", "descr", "Smarket", "leaps", "tidyverse", "ggplot2", "ggthemes", "caret", "BiocManager")

install.packages('BiocManager')
```

```{r}
## Make sure libraries are loading. Had some trouble with this. 
require(ISLR)
require(MASS)
require(descr)
attach(Smarket)

## Linear Discriminant Analysis
freq(Direction)
train = Year<2005
lda.fit=lda(Direction~Lag1+Lag2,data=Smarket, subset=Year<2005)
lda.fit
plot(lda.fit, col="dodgerblue")
Smarket.2005=subset(Smarket,Year==2005) # Creating subset with 2005 data for prediction
lda.pred=predict(lda.fit,Smarket.2005)
names(lda.pred)
lda.class=lda.pred$class
Direction.2005=Smarket$Direction[!train] 
table(lda.class,Direction.2005) 
data.frame(lda.pred)[1:5,]
table(lda.pred$class,Smarket.2005$Direction)
mean(lda.pred$class==Smarket.2005$Direction)
```

## Assignment Questions

From the three methods (best subset, forward stepwise, and backward stepwise):

1.  Which of the three models with k predictors has the smallest training RSS?
    -   Best subset selection has the smallest training RSS. Forward stepwise and backward stepwise have path dependence which can result in an early bad selection impacting the quality of the model overall.
2.  Which of the three models with k predictors has the smallest test RSS?
    -   It depends, because it is possible that all approaches end up with the same model or the forward and backward stepwise select a better model. Generally speaking, again best subset has the smallest test RSS because it considers every model given the input predictors.

## Application Exercise

```{r}
#Generate simulated data 
set.seed(1)
X <- rnorm(100)
eps <- rnorm(100)

```

```{r}
#Generate a response vector y of length n = 100 according to the model
Y <- 3 + 1*X + 4*X^2 - 1*X^3 + eps
```

```{r}
#Use the leaps package 
library(leaps)
```

```{r}
#Use regsubsets() function to perform best subset selection
fit1 <-regsubsets(Y~poly(X,10,raw=T), data=data.frame(Y,X), nvmax=10)

summary1 <-summary(fit1)
```

```{r}
#What is the best model according to Cp, BIC, and adjust R^2? Show plots and report coefficients. 

data_frame(Cp = summary1$cp,
           BIC = summary1$bic,
           AdjR2 = summary1$adjr2) %>%
    mutate(id = row_number()) %>%
    gather(value_type, value, -id) %>%
    ggplot(aes(id, value, col = value_type)) +
    geom_line() + geom_point() + ylab('') + xlab('# of variables') +
    facet_wrap(~ value_type, scales = 'free') +
    theme_tufte() + scale_x_continuous(breaks = 1:10)

par(mfrow = c(2,2))
plot(summary1$rss, xlab = "# of variables", 
     ylab = "RSS", type = "b")
which.min(summary1$rss)
points(11, summary1$rss[10], col = "darkred", cex = 2, pch = 20)


plot(summary1$adjr2, xlab = "# of variables",
     ylab = "Adjusted RSq", type = "b")
which.max(summary1$adjr2)
points(3, summary1$adjr2[10], col = "darkred", cex = 2, pch = 20)

plot(summary1$cp, xlab = "# of variables", 
     ylab = "CP", type = "b")
which.min(summary1$cp)
points(3, summary1$cp[10], col = "darkred", cex = 2, pch = 20)

plot(summary1$bic, xlab = "# of variables", 
     ylab = "BIC", type = "b")
which.min(summary1$bic)
points(3, summary1$bic[10], col = "darkred", cex = 2, pch = 20)
```

3 is the optimal number of variables to include as predictors.

```{r}
#Repeat 3, using forward stepwise selection and backward stepwise selection
library(caret)
#After much troubleshooting, caret would not load successfully on my computer. When running through posit, it did work and I was able to confirm that 3 variables is the optimal model. 


#backward stepwise selection
#backmodel <- train(Y ~ poly(X, 10), data = data.frame, 
#                    method = 'glmStepAIC', direction = 'backward', 
#                    trace = 0,
#               trControl = trainControl(method = 'none', verboseIter = FALSE))

#postResample(predict(backmodel, data.frame), data.frame$Y)

#summary(backmodel$finalModel)


#forward stepwise selection 
#x_poly <- poly(df$X, 10)

#colnames(x_poly) <- paste0('poly', 1:10)
#formodel <- train(y = Y, x = x_poly,
#                    method = 'glmStepAIC', direction = 'forward',
#                    trace = 0,
#               trControl = trainControl(method = 'none', verboseIter = FALSE))

#postResample(predict(formodel, data.frame(x_poly)), df$Y)
```

Forward stepwise model comes to the same conclusion as the best subset and the backward model. According to each method, 3 parameters is optimal.
